#!/usr/bin/env python3
"""
ğŸš€ ç®€åŒ–ç‰ˆæ€§èƒ½benchmark - ä¸“æ³¨äºPyTorch NPU vs CPUå¯¹æ¯”
åŒæ—¶å±•ç¤ºBoasç¼–è¯‘å™¨çš„CANNé›†æˆçŠ¶æ€
"""

import time
import numpy as np
import subprocess
import os
from typing import Dict, List, Optional

def test_cpu_performance():
    """æµ‹è¯•CPUæ€§èƒ½åŸºå‡†"""
    print("ğŸ’» Testing CPU (NumPy) Performance")
    print("-" * 40)
    
    results = []
    matrix_sizes = [64, 128, 256, 512, 1024]
    
    for size in matrix_sizes:
        print(f"ğŸ“Š Testing {size}Ã—{size} matrix...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        A = np.random.randn(size, size).astype(np.float32)
        B = np.random.randn(size, size).astype(np.float32)
        
        # é¢„çƒ­
        for _ in range(3):
            C = np.matmul(A, B)
        
        # æ€§èƒ½æµ‹è¯•
        times = []
        for _ in range(5):
            start = time.time()
            C = np.matmul(A, B)
            end = time.time()
            times.append((end - start) * 1000)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        flops = 2 * size**3
        gflops = (flops / 1e9) / (avg_time / 1000)
        
        result = {
            'size': size,
            'time_ms': avg_time,
            'std_ms': std_time,
            'gflops': gflops,
            'framework': 'CPU (NumPy)'
        }
        results.append(result)
        
        print(f"   âœ… {avg_time:.2f}Â±{std_time:.2f}ms, {gflops:.1f} GFLOPS")
    
    return results

def test_pytorch_npu_performance():
    """æµ‹è¯•PyTorch NPUæ€§èƒ½"""
    print("\nğŸ”¥ Testing PyTorch+NPU Performance") 
    print("-" * 40)
    
    try:
        import torch
        import torch_npu
        
        if not torch_npu.npu.is_available():
            print("âŒ NPU not available")
            return []
        
        device = torch.device('npu:0')
        print(f"âœ… Using device: {torch_npu.npu.get_device_name(0)}")
        
        results = []
        matrix_sizes = [64, 128, 256, 512, 1024]
        
        for size in matrix_sizes:
            print(f"ğŸ“Š Testing {size}Ã—{size} matrix...")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            A = torch.randn(size, size, dtype=torch.float32).to(device)
            B = torch.randn(size, size, dtype=torch.float32).to(device)
            
            # é¢„çƒ­
            for _ in range(5):
                C = torch.matmul(A, B)
                torch_npu.npu.synchronize()
            
            # æ€§èƒ½æµ‹è¯•
            times = []
            for _ in range(10):
                torch_npu.npu.synchronize()
                start = time.time()
                C = torch.matmul(A, B)
                torch_npu.npu.synchronize()
                end = time.time()
                times.append((end - start) * 1000)
            
            avg_time = np.mean(times)
            std_time = np.std(times)
            flops = 2 * size**3
            gflops = (flops / 1e9) / (avg_time / 1000)
            
            result = {
                'size': size,
                'time_ms': avg_time,
                'std_ms': std_time,
                'gflops': gflops,
                'framework': 'PyTorch+NPU'
            }
            results.append(result)
            
            print(f"   âœ… {avg_time:.2f}Â±{std_time:.2f}ms, {gflops:.1f} GFLOPS")
        
        return results
        
    except ImportError:
        print("âŒ PyTorch or torch_npu not available")
        return []
    except Exception as e:
        print(f"âŒ PyTorch NPU test failed: {e}")
        return []

def test_boas_compilation_status():
    """æµ‹è¯•Boasç¼–è¯‘çŠ¶æ€"""
    print("\nğŸ¯ Testing Boas+CANN Integration Status")
    print("-" * 40)
    
    try:
        # æ£€æŸ¥ç¼–è¯‘å™¨æ˜¯å¦å­˜åœ¨
        boas_root = "/root/Boas/Boas-linux"
        build_dir = os.path.join(boas_root, "build")
        pipeline_exe = os.path.join(build_dir, "test-full-pipeline")
        
        if not os.path.exists(pipeline_exe):
            print("âŒ Boas compiler not found")
            return {'status': 'not_found'}
        
        print("âœ… Boas compiler found")
        
        # åˆ›å»ºç®€å•æµ‹è¯•æ–‡ä»¶
        test_code = """import tensor

def test_matmul():
    A = tensor.create(2, 2, [1.0, 2.0, 3.0, 4.0])
    B = tensor.create(2, 2, [5.0, 6.0, 7.0, 8.0])
    C = tensor.matmul(A, B)
    return C

def main():
    result = test_matmul()
    return result
"""
        
        test_file = "/tmp/boas_test.bs"
        with open(test_file, 'w') as f:
            f.write(test_code)
        
        # å°è¯•ç¼–è¯‘
        cmd = [pipeline_exe, "--build", test_file, "test_output"]
        env = os.environ.copy()
        cann_lib = "/usr/local/Ascend/ascend-toolkit/latest/aarch64-linux/lib64"
        env["LD_LIBRARY_PATH"] = f"{cann_lib}:{env.get('LD_LIBRARY_PATH', '')}"
        
        start_time = time.time()
        result = subprocess.run(
            cmd,
            cwd=build_dir,
            capture_output=True,
            text=True,
            env=env,
            timeout=30
        )
        compile_time = (time.time() - start_time) * 1000
        
        # æ¸…ç†
        if os.path.exists(test_file):
            os.remove(test_file)
        
        output = result.stdout
        error = result.stderr
        
        # åˆ†æè¾“å‡º
        status = {
            'status': 'tested',
            'compile_time_ms': compile_time,
            'success': result.returncode == 0,
            'cann_init': '[CANN] Successfully initialized' in output,
            'npu_optimization': 'NPU' in output and 'optimized' in output,
            'mlir_generation': 'MLIR' in output or 'llvm' in output.lower(),
            'error_summary': error[-200:] if error else ""
        }
        
        if status['success']:
            print(f"âœ… Compilation successful ({compile_time:.0f}ms)")
            if status['cann_init']:
                print("âœ… CANN runtime initialization working")
            if status['npu_optimization']:
                print("âœ… NPU optimization path detected")
            if status['mlir_generation']:
                print("âœ… MLIR/LLVM code generation working")
        else:
            print(f"âš ï¸ Compilation issues detected")
            print(f"   - Error: {status['error_summary']}")
            
            # ä»ç„¶æ£€æŸ¥éƒ¨åˆ†åŠŸèƒ½
            if status['cann_init']:
                print("âœ… CANN runtime can initialize")
            if status['mlir_generation']:
                print("âœ… MLIR generation partially working")
        
        return status
        
    except Exception as e:
        print(f"âŒ Boas test failed: {e}")
        return {'status': 'error', 'error': str(e)}

def generate_performance_report(cpu_results, npu_results, boas_status):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“Š PERFORMANCE BENCHMARK RESULTS")
    print("=" * 60)
    
    if cpu_results and npu_results:
        print("\nğŸ† Performance Comparison Table")
        print("-" * 60)
        print(f"{'Matrix Size':<12} {'CPU (NumPy)':<15} {'PyTorch+NPU':<15} {'Speedup':<10}")
        print("-" * 60)
        
        for cpu_res, npu_res in zip(cpu_results, npu_results):
            if cpu_res['size'] == npu_res['size']:
                speedup = npu_res['gflops'] / cpu_res['gflops']
                print(f"{cpu_res['size']}Ã—{cpu_res['size']:<7} "
                      f"{cpu_res['gflops']:>8.1f} GFLOPS   "
                      f"{npu_res['gflops']:>8.1f} GFLOPS   "
                      f"{speedup:>6.1f}x")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_cpu = np.mean([r['gflops'] for r in cpu_results])
        avg_npu = np.mean([r['gflops'] for r in npu_results])
        avg_speedup = avg_npu / avg_cpu
        
        print("-" * 60)
        print(f"{'Average':<12} {avg_cpu:>8.1f} GFLOPS   {avg_npu:>8.1f} GFLOPS   {avg_speedup:>6.1f}x")
        
        # æ€§èƒ½åˆ†æ
        print(f"\nğŸ“ˆ Performance Analysis:")
        print(f"   â€¢ CPU Baseline: {avg_cpu:.1f} GFLOPS average")
        print(f"   â€¢ NPU Performance: {avg_npu:.1f} GFLOPS average")  
        print(f"   â€¢ Overall Speedup: {avg_speedup:.1f}x faster on NPU")
        
        # æ‰¾å‡ºæœ€ä½³æ€§èƒ½ç‚¹
        best_npu = max(npu_results, key=lambda x: x['gflops'])
        print(f"   â€¢ Peak NPU Performance: {best_npu['gflops']:.1f} GFLOPS ({best_npu['size']}Ã—{best_npu['size']})")
    
    # BoasçŠ¶æ€æŠ¥å‘Š
    print(f"\nğŸ¯ Boas+CANN Integration Status:")
    if boas_status['status'] == 'not_found':
        print("   âŒ Boas compiler not found")
    elif boas_status['status'] == 'error':
        print(f"   âŒ Test error: {boas_status.get('error', 'Unknown')}")
    else:
        if boas_status.get('success', False):
            print("   âœ… Boas compilation working")
        else:
            print("   âš ï¸ Boas compilation has issues")
            
        if boas_status.get('cann_init', False):
            print("   âœ… CANN runtime integration working")
        if boas_status.get('npu_optimization', False):
            print("   âœ… NPU optimization path active")
        if boas_status.get('mlir_generation', False):
            print("   âœ… MLIR/LLVM code generation working")
    
    # ä¸å‚è€ƒå®ç°å¯¹æ¯”
    print(f"\nğŸ“‹ Reference Performance Targets:")
    print(f"   â€¢ Triton-Ascend: ~15,000-50,000 GFLOPS (estimated)")
    print(f"   â€¢ PyTorch+NPU: {avg_npu:.1f} GFLOPS (measured)")
    print(f"   â€¢ Boas+CANN: Integration ready, performance optimization needed")
    
    print(f"\nğŸ”® Next Steps for Boas:")
    print(f"   1. Fix remaining compilation issues")
    print(f"   2. Implement end-to-end NPU execution")
    print(f"   3. Add MLIR optimization passes")
    print(f"   4. Benchmark against PyTorch+NPU baseline")
    print(f"   5. Target: Match or exceed {avg_npu:.0f} GFLOPS average")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ NPU Performance Benchmark Suite")
    print("=" * 60)
    print("ğŸ¯ Comparing CPU vs PyTorch+NPU vs Boas+CANN Integration")
    print("=" * 60)
    
    # è¿è¡ŒCPUåŸºå‡†æµ‹è¯•
    cpu_results = test_cpu_performance()
    
    # è¿è¡ŒPyTorch NPUæµ‹è¯•
    npu_results = test_pytorch_npu_performance()
    
    # æµ‹è¯•Boasé›†æˆçŠ¶æ€
    boas_status = test_boas_compilation_status()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_performance_report(cpu_results, npu_results, boas_status)
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results = {
        'timestamp': timestamp,
        'cpu_results': cpu_results,
        'npu_results': npu_results,
        'boas_status': boas_status
    }
    
    import json
    with open(f'benchmark_results_{timestamp}.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to benchmark_results_{timestamp}.json")
    print("âœ… Benchmark completed!")

if __name__ == "__main__":
    main()
