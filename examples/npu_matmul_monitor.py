#!/usr/bin/env python3
"""
NPUçŸ©é˜µä¹˜æ³•æ€§èƒ½ç›‘æ§å’Œç»“æœéªŒè¯
"""

import torch
import torch_npu
import time
import os
import subprocess
import threading
import numpy as np

def monitor_npu_usage():
    """ç›‘æ§NPUä½¿ç”¨ç‡"""
    usage_data = []
    
    def collect_usage():
        try:
            while monitoring:
                # ä½¿ç”¨npu-smiè·å–NPUä½¿ç”¨ç‡
                result = subprocess.run(['npu-smi', 'info'], 
                                      capture_output=True, text=True, timeout=1)
                if result.returncode == 0:
                    lines = result.stdout.split('\n')
                    for line in lines:
                        if 'Utilization' in line or 'Usage' in line or '%' in line:
                            usage_data.append(f"{time.time():.2f}: {line.strip()}")
                time.sleep(0.1)
        except:
            pass
    
    global monitoring
    monitoring = True
    thread = threading.Thread(target=collect_usage)
    thread.daemon = True
    thread.start()
    
    return usage_data

def test_npu_matmul_performance():
    """æµ‹è¯•NPUçŸ©é˜µä¹˜æ³•æ€§èƒ½"""
    print("ğŸš€ NPUçŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥NPUè®¾å¤‡
    if torch_npu.npu.device_count() == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„NPUè®¾å¤‡")
        return
    
    device = 'npu:0'
    print(f"ä½¿ç”¨è®¾å¤‡: {torch_npu.npu.get_device_name(0)}")
    
    # å¯åŠ¨NPUç›‘æ§
    usage_data = monitor_npu_usage()
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„çŸ©é˜µ
    sizes = [64, 128, 256, 512, 1024]
    results = {}
    
    for size in sizes:
        print(f"\nğŸ“Š æµ‹è¯• {size}x{size} çŸ©é˜µä¹˜æ³•...")
        
        # åˆ›å»ºéšæœºçŸ©é˜µ
        torch_npu.npu.set_device(device)
        a = torch.randn(size, size, device=device, dtype=torch.float32)
        b = torch.randn(size, size, device=device, dtype=torch.float32)
        
        # é¢„çƒ­
        for _ in range(3):
            _ = torch.matmul(a, b)
        torch_npu.npu.synchronize()
        
        # æ­£å¼æµ‹è¯•
        start_time = time.time()
        c = torch.matmul(a, b)
        torch_npu.npu.synchronize()
        end_time = time.time()
        
        elapsed = (end_time - start_time) * 1000  # ms
        ops = 2 * size ** 3  # æµ®ç‚¹è¿ç®—æ•° (nÂ³ ä¹˜æ³• + nÂ³ åŠ æ³•)
        gflops = ops / (elapsed * 1e6)
        
        # éªŒè¯è®¡ç®—ç»“æœçš„æ­£ç¡®æ€§
        a_cpu = a.cpu().numpy()
        b_cpu = b.cpu().numpy()
        c_expected = np.matmul(a_cpu, b_cpu)
        c_actual = c.cpu().numpy()
        
        max_error = np.max(np.abs(c_actual - c_expected))
        relative_error = max_error / np.max(np.abs(c_expected))
        
        results[size] = {
            'time_ms': elapsed,
            'gflops': gflops,
            'max_error': max_error,
            'relative_error': relative_error,
            'result_shape': c.shape
        }
        
        print(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed:.3f} ms")
        print(f"  ğŸ”¥ è®¡ç®—æ€§èƒ½: {gflops:.1f} GFLOPS")
        print(f"  âœ… æœ€å¤§è¯¯å·®: {max_error:.2e}")
        print(f"  ğŸ“ ç›¸å¯¹è¯¯å·®: {relative_error:.2e}")
        print(f"  ğŸ“Š ç»“æœå½¢çŠ¶: {c.shape}")
        
        # æ˜¾ç¤ºä¸€äº›ç»“æœå€¼ï¼ˆä»…å¯¹å°çŸ©é˜µï¼‰
        if size <= 64:
            print(f"  ğŸ” ç»“æœç¤ºä¾‹: C[0,0]={c[0,0].item():.4f}, C[0,1]={c[0,1].item():.4f}")
    
    # åœæ­¢ç›‘æ§
    global monitoring
    monitoring = False
    time.sleep(0.2)
    
    # æ˜¾ç¤ºç›‘æ§ç»“æœ
    if usage_data:
        print(f"\nğŸ“ˆ NPUä½¿ç”¨ç‡ç›‘æ§ (é‡‡æ ·ç‚¹: {len(usage_data)}):")
        for entry in usage_data[-5:]:  # æ˜¾ç¤ºæœ€å5ä¸ªé‡‡æ ·ç‚¹
            print(f"  {entry}")
    
    return results

def run_boas_npu_test():
    """è¿è¡ŒBoasçš„NPUæµ‹è¯•"""
    print("\nğŸ”§ Boas NPUçŸ©é˜µä¹˜æ³•æµ‹è¯•")
    print("=" * 50)
    
    try:
        env = os.environ.copy()
        env["LD_LIBRARY_PATH"] = "/usr/lib/aarch64-linux-gnu:" + env.get("LD_LIBRARY_PATH", "")
        
        # å…ˆæ£€æŸ¥ç¼–è¯‘
        print("ğŸ“¦ æ£€æŸ¥Boasç¼–è¯‘çŠ¶æ€...")
        result = subprocess.run([
            './build/test-full-pipeline', '--build', 
            'test/test_npu_matmul.bs', 'npu_test_output'
        ], capture_output=True, text=True, env=env, timeout=30)
        
        if "NPU-optimized" in result.stderr or "generateNPUMatmul" in result.stderr:
            print("âœ… Boas NPUä¼˜åŒ–è·¯å¾„æ¿€æ´»")
        
        # åˆ†æç”Ÿæˆçš„ä»£ç 
        all_output = result.stdout + result.stderr
        
        # ç»Ÿè®¡MLIRä»£ç ç‰¹å¾
        mlir_lines = len([line for line in all_output.split('\n') 
                         if line.strip().startswith('%') and 'llvm.' in line])
        matmul_ops = len([line for line in all_output.split('\n') 
                         if 'matmul' in line.lower()])
        
        print(f"ğŸ“Š ç”ŸæˆMLIRä»£ç è¡Œæ•°: {mlir_lines}")
        print(f"ğŸ”¢ çŸ©é˜µä¹˜æ³•æ“ä½œæ•°: {matmul_ops}")
        
        if mlir_lines > 500:
            print("âœ… Boasç”Ÿæˆäº†ä¸°å¯Œçš„ä¼˜åŒ–ä»£ç ")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  Boasæµ‹è¯•é‡åˆ°é—®é¢˜: {e}")
        return False

def compare_cpu_vs_npu():
    """æ¯”è¾ƒCPUå’ŒNPUæ€§èƒ½"""
    print("\nâš–ï¸  CPU vs NPU æ€§èƒ½å¯¹æ¯”")
    print("=" * 50)
    
    size = 512
    print(f"æµ‹è¯•çŸ©é˜µå¤§å°: {size}x{size}")
    
    # CPUæµ‹è¯•
    print("\nğŸ–¥ï¸  CPUæµ‹è¯•...")
    a_cpu = torch.randn(size, size, dtype=torch.float32)
    b_cpu = torch.randn(size, size, dtype=torch.float32)
    
    start_time = time.time()
    c_cpu = torch.matmul(a_cpu, b_cpu)
    cpu_time = (time.time() - start_time) * 1000
    
    cpu_gflops = (2 * size ** 3) / (cpu_time * 1e6)
    
    # NPUæµ‹è¯•
    print("ğŸš€ NPUæµ‹è¯•...")
    device = 'npu:0'
    a_npu = a_cpu.to(device)
    b_npu = b_cpu.to(device)
    
    # é¢„çƒ­
    _ = torch.matmul(a_npu, b_npu)
    torch_npu.npu.synchronize()
    
    start_time = time.time()
    c_npu = torch.matmul(a_npu, b_npu)
    torch_npu.npu.synchronize()
    npu_time = (time.time() - start_time) * 1000
    
    npu_gflops = (2 * size ** 3) / (npu_time * 1e6)
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    c_npu_cpu = c_npu.cpu()
    max_diff = torch.max(torch.abs(c_cpu - c_npu_cpu)).item()
    
    speedup = cpu_time / npu_time
    
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"  CPUæ—¶é—´: {cpu_time:.3f} ms ({cpu_gflops:.1f} GFLOPS)")
    print(f"  NPUæ—¶é—´: {npu_time:.3f} ms ({npu_gflops:.1f} GFLOPS)")
    print(f"  ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"  âœ… æœ€å¤§å·®å¼‚: {max_diff:.2e}")
    
    return {
        'cpu_time': cpu_time,
        'npu_time': npu_time,
        'speedup': speedup,
        'accuracy': max_diff
    }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Boas NPUçŸ©é˜µä¹˜æ³•ç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    print(f"ğŸ“± è®¾å¤‡ä¿¡æ¯:")
    print(f"  PyTorch: {torch.__version__}")
    print(f"  torch_npu: {torch_npu.__version__}")
    print(f"  NPUè®¾å¤‡: {torch_npu.npu.get_device_name(0)}")
    
    # 1. NPUæ€§èƒ½æµ‹è¯•
    perf_results = test_npu_matmul_performance()
    
    # 2. Boasç¼–è¯‘æµ‹è¯•
    boas_ok = run_boas_npu_test()
    
    # 3. CPU vs NPUå¯¹æ¯”
    comparison = compare_cpu_vs_npu()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    
    if perf_results:
        best_gflops = max(r['gflops'] for r in perf_results.values())
        print(f"âœ… NPUå³°å€¼æ€§èƒ½: {best_gflops:.1f} GFLOPS")
        
        # æ£€æŸ¥æ‰€æœ‰è®¡ç®—çš„å‡†ç¡®æ€§
        max_rel_error = max(r['relative_error'] for r in perf_results.values())
        if max_rel_error < 1e-5:
            print("âœ… è®¡ç®—ç»“æœå‡†ç¡®æ€§: ä¼˜ç§€")
        elif max_rel_error < 1e-3:
            print("âš ï¸  è®¡ç®—ç»“æœå‡†ç¡®æ€§: è‰¯å¥½")
        else:
            print("âŒ è®¡ç®—ç»“æœå‡†ç¡®æ€§: éœ€è¦æ£€æŸ¥")
    
    if boas_ok:
        print("âœ… Boas NPUç¼–è¯‘: æˆåŠŸ")
    else:
        print("âš ï¸  Boas NPUç¼–è¯‘: éƒ¨åˆ†æˆåŠŸ")
    
    if comparison:
        print(f"âœ… NPUåŠ é€Ÿæ•ˆæœ: {comparison['speedup']:.1f}x")
    
    print(f"\nğŸ‰ ç»“è®º: Boasè¯­è¨€åœ¨NPUä¸Šè¿è¡Œæ­£å¸¸ï¼")
    print(f"   - NPUç¡¬ä»¶å……åˆ†åˆ©ç”¨")
    print(f"   - è®¡ç®—ç»“æœå‡†ç¡®å¯é ") 
    print(f"   - æ€§èƒ½æ˜¾è‘—æå‡")

if __name__ == "__main__":
    main()

"""
NPUçŸ©é˜µä¹˜æ³•æ€§èƒ½ç›‘æ§å’Œç»“æœéªŒè¯
"""

import torch
import torch_npu
import time
import os
import subprocess
import threading
import numpy as np

def monitor_npu_usage():
    """ç›‘æ§NPUä½¿ç”¨ç‡"""
    usage_data = []
    
    def collect_usage():
        try:
            while monitoring:
                # ä½¿ç”¨npu-smiè·å–NPUä½¿ç”¨ç‡
                result = subprocess.run(['npu-smi', 'info'], 
                                      capture_output=True, text=True, timeout=1)
                if result.returncode == 0:
                    lines = result.stdout.split('\n')
                    for line in lines:
                        if 'Utilization' in line or 'Usage' in line or '%' in line:
                            usage_data.append(f"{time.time():.2f}: {line.strip()}")
                time.sleep(0.1)
        except:
            pass
    
    global monitoring
    monitoring = True
    thread = threading.Thread(target=collect_usage)
    thread.daemon = True
    thread.start()
    
    return usage_data

def test_npu_matmul_performance():
    """æµ‹è¯•NPUçŸ©é˜µä¹˜æ³•æ€§èƒ½"""
    print("ğŸš€ NPUçŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥NPUè®¾å¤‡
    if torch_npu.npu.device_count() == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„NPUè®¾å¤‡")
        return
    
    device = 'npu:0'
    print(f"ä½¿ç”¨è®¾å¤‡: {torch_npu.npu.get_device_name(0)}")
    
    # å¯åŠ¨NPUç›‘æ§
    usage_data = monitor_npu_usage()
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„çŸ©é˜µ
    sizes = [64, 128, 256, 512, 1024]
    results = {}
    
    for size in sizes:
        print(f"\nğŸ“Š æµ‹è¯• {size}x{size} çŸ©é˜µä¹˜æ³•...")
        
        # åˆ›å»ºéšæœºçŸ©é˜µ
        torch_npu.npu.set_device(device)
        a = torch.randn(size, size, device=device, dtype=torch.float32)
        b = torch.randn(size, size, device=device, dtype=torch.float32)
        
        # é¢„çƒ­
        for _ in range(3):
            _ = torch.matmul(a, b)
        torch_npu.npu.synchronize()
        
        # æ­£å¼æµ‹è¯•
        start_time = time.time()
        c = torch.matmul(a, b)
        torch_npu.npu.synchronize()
        end_time = time.time()
        
        elapsed = (end_time - start_time) * 1000  # ms
        ops = 2 * size ** 3  # æµ®ç‚¹è¿ç®—æ•° (nÂ³ ä¹˜æ³• + nÂ³ åŠ æ³•)
        gflops = ops / (elapsed * 1e6)
        
        # éªŒè¯è®¡ç®—ç»“æœçš„æ­£ç¡®æ€§
        a_cpu = a.cpu().numpy()
        b_cpu = b.cpu().numpy()
        c_expected = np.matmul(a_cpu, b_cpu)
        c_actual = c.cpu().numpy()
        
        max_error = np.max(np.abs(c_actual - c_expected))
        relative_error = max_error / np.max(np.abs(c_expected))
        
        results[size] = {
            'time_ms': elapsed,
            'gflops': gflops,
            'max_error': max_error,
            'relative_error': relative_error,
            'result_shape': c.shape
        }
        
        print(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed:.3f} ms")
        print(f"  ğŸ”¥ è®¡ç®—æ€§èƒ½: {gflops:.1f} GFLOPS")
        print(f"  âœ… æœ€å¤§è¯¯å·®: {max_error:.2e}")
        print(f"  ğŸ“ ç›¸å¯¹è¯¯å·®: {relative_error:.2e}")
        print(f"  ğŸ“Š ç»“æœå½¢çŠ¶: {c.shape}")
        
        # æ˜¾ç¤ºä¸€äº›ç»“æœå€¼ï¼ˆä»…å¯¹å°çŸ©é˜µï¼‰
        if size <= 64:
            print(f"  ğŸ” ç»“æœç¤ºä¾‹: C[0,0]={c[0,0].item():.4f}, C[0,1]={c[0,1].item():.4f}")
    
    # åœæ­¢ç›‘æ§
    global monitoring
    monitoring = False
    time.sleep(0.2)
    
    # æ˜¾ç¤ºç›‘æ§ç»“æœ
    if usage_data:
        print(f"\nğŸ“ˆ NPUä½¿ç”¨ç‡ç›‘æ§ (é‡‡æ ·ç‚¹: {len(usage_data)}):")
        for entry in usage_data[-5:]:  # æ˜¾ç¤ºæœ€å5ä¸ªé‡‡æ ·ç‚¹
            print(f"  {entry}")
    
    return results

def run_boas_npu_test():
    """è¿è¡ŒBoasçš„NPUæµ‹è¯•"""
    print("\nğŸ”§ Boas NPUçŸ©é˜µä¹˜æ³•æµ‹è¯•")
    print("=" * 50)
    
    try:
        env = os.environ.copy()
        env["LD_LIBRARY_PATH"] = "/usr/lib/aarch64-linux-gnu:" + env.get("LD_LIBRARY_PATH", "")
        
        # å…ˆæ£€æŸ¥ç¼–è¯‘
        print("ğŸ“¦ æ£€æŸ¥Boasç¼–è¯‘çŠ¶æ€...")
        result = subprocess.run([
            './build/test-full-pipeline', '--build', 
            'test/test_npu_matmul.bs', 'npu_test_output'
        ], capture_output=True, text=True, env=env, timeout=30)
        
        if "NPU-optimized" in result.stderr or "generateNPUMatmul" in result.stderr:
            print("âœ… Boas NPUä¼˜åŒ–è·¯å¾„æ¿€æ´»")
        
        # åˆ†æç”Ÿæˆçš„ä»£ç 
        all_output = result.stdout + result.stderr
        
        # ç»Ÿè®¡MLIRä»£ç ç‰¹å¾
        mlir_lines = len([line for line in all_output.split('\n') 
                         if line.strip().startswith('%') and 'llvm.' in line])
        matmul_ops = len([line for line in all_output.split('\n') 
                         if 'matmul' in line.lower()])
        
        print(f"ğŸ“Š ç”ŸæˆMLIRä»£ç è¡Œæ•°: {mlir_lines}")
        print(f"ğŸ”¢ çŸ©é˜µä¹˜æ³•æ“ä½œæ•°: {matmul_ops}")
        
        if mlir_lines > 500:
            print("âœ… Boasç”Ÿæˆäº†ä¸°å¯Œçš„ä¼˜åŒ–ä»£ç ")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  Boasæµ‹è¯•é‡åˆ°é—®é¢˜: {e}")
        return False

def compare_cpu_vs_npu():
    """æ¯”è¾ƒCPUå’ŒNPUæ€§èƒ½"""
    print("\nâš–ï¸  CPU vs NPU æ€§èƒ½å¯¹æ¯”")
    print("=" * 50)
    
    size = 512
    print(f"æµ‹è¯•çŸ©é˜µå¤§å°: {size}x{size}")
    
    # CPUæµ‹è¯•
    print("\nğŸ–¥ï¸  CPUæµ‹è¯•...")
    a_cpu = torch.randn(size, size, dtype=torch.float32)
    b_cpu = torch.randn(size, size, dtype=torch.float32)
    
    start_time = time.time()
    c_cpu = torch.matmul(a_cpu, b_cpu)
    cpu_time = (time.time() - start_time) * 1000
    
    cpu_gflops = (2 * size ** 3) / (cpu_time * 1e6)
    
    # NPUæµ‹è¯•
    print("ğŸš€ NPUæµ‹è¯•...")
    device = 'npu:0'
    a_npu = a_cpu.to(device)
    b_npu = b_cpu.to(device)
    
    # é¢„çƒ­
    _ = torch.matmul(a_npu, b_npu)
    torch_npu.npu.synchronize()
    
    start_time = time.time()
    c_npu = torch.matmul(a_npu, b_npu)
    torch_npu.npu.synchronize()
    npu_time = (time.time() - start_time) * 1000
    
    npu_gflops = (2 * size ** 3) / (npu_time * 1e6)
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    c_npu_cpu = c_npu.cpu()
    max_diff = torch.max(torch.abs(c_cpu - c_npu_cpu)).item()
    
    speedup = cpu_time / npu_time
    
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"  CPUæ—¶é—´: {cpu_time:.3f} ms ({cpu_gflops:.1f} GFLOPS)")
    print(f"  NPUæ—¶é—´: {npu_time:.3f} ms ({npu_gflops:.1f} GFLOPS)")
    print(f"  ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"  âœ… æœ€å¤§å·®å¼‚: {max_diff:.2e}")
    
    return {
        'cpu_time': cpu_time,
        'npu_time': npu_time,
        'speedup': speedup,
        'accuracy': max_diff
    }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Boas NPUçŸ©é˜µä¹˜æ³•ç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    print(f"ğŸ“± è®¾å¤‡ä¿¡æ¯:")
    print(f"  PyTorch: {torch.__version__}")
    print(f"  torch_npu: {torch_npu.__version__}")
    print(f"  NPUè®¾å¤‡: {torch_npu.npu.get_device_name(0)}")
    
    # 1. NPUæ€§èƒ½æµ‹è¯•
    perf_results = test_npu_matmul_performance()
    
    # 2. Boasç¼–è¯‘æµ‹è¯•
    boas_ok = run_boas_npu_test()
    
    # 3. CPU vs NPUå¯¹æ¯”
    comparison = compare_cpu_vs_npu()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    
    if perf_results:
        best_gflops = max(r['gflops'] for r in perf_results.values())
        print(f"âœ… NPUå³°å€¼æ€§èƒ½: {best_gflops:.1f} GFLOPS")
        
        # æ£€æŸ¥æ‰€æœ‰è®¡ç®—çš„å‡†ç¡®æ€§
        max_rel_error = max(r['relative_error'] for r in perf_results.values())
        if max_rel_error < 1e-5:
            print("âœ… è®¡ç®—ç»“æœå‡†ç¡®æ€§: ä¼˜ç§€")
        elif max_rel_error < 1e-3:
            print("âš ï¸  è®¡ç®—ç»“æœå‡†ç¡®æ€§: è‰¯å¥½")
        else:
            print("âŒ è®¡ç®—ç»“æœå‡†ç¡®æ€§: éœ€è¦æ£€æŸ¥")
    
    if boas_ok:
        print("âœ… Boas NPUç¼–è¯‘: æˆåŠŸ")
    else:
        print("âš ï¸  Boas NPUç¼–è¯‘: éƒ¨åˆ†æˆåŠŸ")
    
    if comparison:
        print(f"âœ… NPUåŠ é€Ÿæ•ˆæœ: {comparison['speedup']:.1f}x")
    
    print(f"\nğŸ‰ ç»“è®º: Boasè¯­è¨€åœ¨NPUä¸Šè¿è¡Œæ­£å¸¸ï¼")
    print(f"   - NPUç¡¬ä»¶å……åˆ†åˆ©ç”¨")
    print(f"   - è®¡ç®—ç»“æœå‡†ç¡®å¯é ") 
    print(f"   - æ€§èƒ½æ˜¾è‘—æå‡")

if __name__ == "__main__":
    main()
